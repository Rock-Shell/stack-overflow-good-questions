{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "whole-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from torch.nn.functional import cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tested-oregon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34552656</td>\n",
       "      <td>Java: Repeat Task Every Random Seconds</td>\n",
       "      <td>&lt;p&gt;I'm already familiar with repeating tasks e...</td>\n",
       "      <td>&lt;java&gt;&lt;repeat&gt;</td>\n",
       "      <td>2016-01-01 00:21:59</td>\n",
       "      <td>LQ_CLOSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34553034</td>\n",
       "      <td>Why are Java Optionals immutable?</td>\n",
       "      <td>&lt;p&gt;I'd like to understand why Java 8 Optionals...</td>\n",
       "      <td>&lt;java&gt;&lt;optional&gt;</td>\n",
       "      <td>2016-01-01 02:03:20</td>\n",
       "      <td>HQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34553174</td>\n",
       "      <td>Text Overlay Image with Darkened Opacity React...</td>\n",
       "      <td>&lt;p&gt;I am attempting to overlay a title over an ...</td>\n",
       "      <td>&lt;javascript&gt;&lt;image&gt;&lt;overlay&gt;&lt;react-native&gt;&lt;opa...</td>\n",
       "      <td>2016-01-01 02:48:24</td>\n",
       "      <td>HQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34553318</td>\n",
       "      <td>Why ternary operator in swift is so picky?</td>\n",
       "      <td>&lt;p&gt;The question is very simple, but I just cou...</td>\n",
       "      <td>&lt;swift&gt;&lt;operators&gt;&lt;whitespace&gt;&lt;ternary-operato...</td>\n",
       "      <td>2016-01-01 03:30:17</td>\n",
       "      <td>HQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34553755</td>\n",
       "      <td>hide/show fab with scale animation</td>\n",
       "      <td>&lt;p&gt;I'm using custom floatingactionmenu. I need...</td>\n",
       "      <td>&lt;android&gt;&lt;material-design&gt;&lt;floating-action-but...</td>\n",
       "      <td>2016-01-01 05:21:48</td>\n",
       "      <td>HQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              Title  \\\n",
       "0  34552656             Java: Repeat Task Every Random Seconds   \n",
       "1  34553034                  Why are Java Optionals immutable?   \n",
       "2  34553174  Text Overlay Image with Darkened Opacity React...   \n",
       "3  34553318         Why ternary operator in swift is so picky?   \n",
       "4  34553755                 hide/show fab with scale animation   \n",
       "\n",
       "                                                Body  \\\n",
       "0  <p>I'm already familiar with repeating tasks e...   \n",
       "1  <p>I'd like to understand why Java 8 Optionals...   \n",
       "2  <p>I am attempting to overlay a title over an ...   \n",
       "3  <p>The question is very simple, but I just cou...   \n",
       "4  <p>I'm using custom floatingactionmenu. I need...   \n",
       "\n",
       "                                                Tags         CreationDate  \\\n",
       "0                                     <java><repeat>  2016-01-01 00:21:59   \n",
       "1                                   <java><optional>  2016-01-01 02:03:20   \n",
       "2  <javascript><image><overlay><react-native><opa...  2016-01-01 02:48:24   \n",
       "3  <swift><operators><whitespace><ternary-operato...  2016-01-01 03:30:17   \n",
       "4  <android><material-design><floating-action-but...  2016-01-01 05:21:48   \n",
       "\n",
       "          Y  \n",
       "0  LQ_CLOSE  \n",
       "1        HQ  \n",
       "2        HQ  \n",
       "3        HQ  \n",
       "4        HQ  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "valid = pd.read_csv(\"valid.csv\")\n",
    "train.head() #45000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "opening-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid.head()# 15k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "seeing-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text=text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '',text)\n",
    "    text = re.sub(r'\\d+', '',text)\n",
    "    text = re.sub('https?:\\/\\/t.co\\/[A-Za-z0-9]+', '', text)\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "#     text=word_tokenize(text)\n",
    "#     stop_words= stopwords.words('english')\n",
    "#     text= [i for i in text if i not in stop_words]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eastern-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Title'] = train['Title'].apply(lambda row: preprocess(row))\n",
    "train['Y'] = train['Y'].apply(lambda row: 0 if row=='HQ' else(1 if row==\"LQ_CLOSE\" else 2))\n",
    "# 0:HW, 1:LQ_CLOSE, 2:LQ_EDIT\n",
    "valid['Title'] = valid['Title'].apply(lambda row: preprocess(row))\n",
    "valid['Y'] = valid['Y'].apply(lambda row: 0 if row=='HQ' else(1 if row==\"LQ_CLOSE\" else 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "touched-relay",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sealed-dylan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x19d8093d9a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = [len(i.split()) for i in train['Title']]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "modular-genius",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2126: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train['Title'].tolist(),\n",
    "    max_length = 20,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "          \n",
    "tokens_valid = tokenizer.batch_encode_plus(\n",
    "    valid['Title'].tolist(),\n",
    "    max_length = 20,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accomplished-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_tokens = torch.tensor(tokens_train['attention_mask'])\n",
    "train_label = torch.tensor(train['Y'].to_list())\n",
    "\n",
    "valid_seq = torch.tensor(tokens_valid['input_ids'])\n",
    "valid_tokens = torch.tensor(tokens_valid['attention_mask'])\n",
    "valid_label = torch.tensor(valid['Y'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cardiac-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train and valid loader\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler,  SequentialSampler\n",
    "batch_size = 32\n",
    "train_data = TensorDataset(train_seq, train_tokens, train_label)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)\n",
    "\n",
    "valid_data = TensorDataset(valid_seq, valid_tokens, valid_label)\n",
    "valid_sampler = RandomSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler = valid_sampler, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "constant-shell",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert_arch(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(Bert_arch, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        self.fc2 = nn.Linear(512,3)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, sent_id, mask):\n",
    "        _,cls_hs = self.bert(sent_id, attention_mask=mask).to_tuple()\n",
    "#         print(\"cls_hs\",_,cls_hs)\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "vertical-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "interim-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bert_arch(bert)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fitted-healing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = [\"This is some text, I want to sleep\", \"hope it runs\"]\n",
    "# sent_id = tokenizer.batch_encode_plus(text, padding=True)\n",
    "# # model(sent_id['input_ids'],sent_id['attention_mask'])\n",
    "# model(sent_id['input_ids'],sent_id['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "rocky-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dedicated-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def training():\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0,0\n",
    "    total_preds = []\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step%50 ==0 and not step == 0:\n",
    "            print(f\" Batch {step} of {len(train_dataloader)}\")\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "#         print(\"sent_id\",type(sent_id))\n",
    "        model.zero_grad() # clear prevously calculated weighs\n",
    "        preds = model(sent_id, mask)\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(),1.0)\n",
    "        # to prevent exploding gradient \n",
    "        \n",
    "        optimizer.step()\n",
    "        preds = preds.detach().cpu().numpy() # push model predictions to cpu\n",
    "        total_preds.append(preds)\n",
    "    avg_loss = total_loss/len(train_dataloader)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "    return avg_loss, total_preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "interracial-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    print(\"Evaluating.....\")\n",
    "    model.eval() # deactivate drop out layers\n",
    "    total_loss,total_accuracy = 0,0\n",
    "    total_preds = []\n",
    "    for step, batch in enumerate(valid_dataloader):\n",
    "        if step%50==0 and not step==0:\n",
    "            print(f\"Valid Batch {step} of {len(valid_dataloader)}\")\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        with torch.no_grad():\n",
    "            loss = cross_entropy(preds,labels)\n",
    "            total_loss += loss\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "    avg_loss = total_loss/ len(val_train)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "    return avg_loss, total_preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "certain-enforcement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-65-f6cf432cf591>:18: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(),1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch 50 of 1407\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-36507a1d6353>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'\\n Epoch {epoch+1}/{epochs}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-f6cf432cf591>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#         print(\"sent_id\",type(sent_id))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# clear prevously calculated weighs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-59-f429682bf265>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, sent_id, mask)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLogSoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcls_hs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;31m#         print(\"cls_hs\",_,cls_hs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls_hs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    989\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    990\u001b[0m         )\n\u001b[1;32m--> 991\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    992\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    580\u001b[0m                 )\n\u001b[0;32m    581\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    583\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    471\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m     ):\n\u001b[1;32m--> 401\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    402\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    287\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m             \u001b[0mkey_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1751\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1753\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\n Epoch {epoch+1}/{epochs}')\n",
    "    train_loss,_ = training()\n",
    "    valid_loss,_ = evaluate()\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(),'saved_weights.pt')\n",
    "        \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f\"Training loss: <{train_loss:.3f}>  Valid loss: <{valid_loss:.3f}>\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-jenny",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
